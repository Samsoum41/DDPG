{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d561c7-f228-47fe-b238-45ed84949b65",
   "metadata": {},
   "source": [
    "# Pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789df808-d345-48c8-8d5a-473a96119b5f",
   "metadata": {},
   "source": [
    "Dans ce notebook, on s'intéressera à la résolution du problème du Pendulum disponible sur la plateforme gymnasium d'OpenAI :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8fedd1-a500-4b16-beb3-d6ec7e8ca5bf",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src = \"https://www.gymlibrary.dev/_images/pendulum.gif\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038671b-c71f-4983-8bad-b8343dce3e2f",
   "metadata": {},
   "source": [
    "Pour résoudre cet problème, nous allons nous intéresser à l'algorithme de Deep Deterministic Gradient Policy (DDPG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "247d7a62-e4f7-4dfb-97ef-03c94777445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ab15ca-c8dc-49c1-8e10-d78dd2804d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"Pendulum-v1\"\n",
    "env = gym.make(problem)\n",
    "#env = gym.make(problem, render_mode = \"human\")\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3279df8f-88f0-4608-9036-a007cd9b58f6",
   "metadata": {},
   "source": [
    "On observe que le problème est décrit à chaque instant par un état contenant 3 variables et une action compris entre [-2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e56850c-1770-4bd7-9ae2-c5d555e36423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille d'un état : \t 3\n",
      "Taille d'une action : \t 1\n",
      "Bornes de l'action : \t [-2.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Taille d'un état : \\t {num_states}\")\n",
    "print(f\"Taille d'une action : \\t {num_actions}\")\n",
    "print(f\"Bornes de l'action : \\t [{lower_bound}, {upper_bound}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0cfbbf-2d2c-4dce-88b0-880b20a04f08",
   "metadata": {},
   "source": [
    "## Bruit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62427e47-ba4a-4649-8580-977cc6ef54f3",
   "metadata": {},
   "source": [
    "Pour piloter l'exploration de notre agent et aussi améliorer sa stabilité, nous utiliserons du bruit pour perturber l'action de la politique à chaque fois. On peut utiliser plusieurs bruits. Ici nous en présenterons 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6287892-76de-42e5-b5dc-0b7a936b19bd",
   "metadata": {},
   "source": [
    "- Le bruit d'Ornstein-Uhlenbeck, solution initiale proposée avec l'algorithme DDPG dans le ARTICLE (Continuous control with deep reinforcement learning, 2015, https://arxiv.org/abs/1509.02971). En effet, le processus d'Ornstein-Uhlenbeck a pour propriété d'être gaussien à chaque instant et à générer des variables aléatoires corrélés. C'est aussi un processus de Markov. Initialement, l'algorithme DDPG ajoute une réalisation de ce processus à chaque action que le réseau de neurone action réalise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b50646-6f2e-4f11-8b42-f1d621455ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3671354-431a-4254-ac33-d97ead17bbc3",
   "metadata": {},
   "source": [
    "- Le bruit gaussien, simplement généré par des variables aléatoires gaussiennes i.i.d, est une autre façon de réaliser du bruit. On ajoute une réalisation d'une gaussienne à l'action suggéré par la politique de l'acteur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dab6a1b-53d4-4c9d-a6ee-9482af7aad27",
   "metadata": {},
   "source": [
    "- Enfin, une dernière approche plus récente serait de perturber l'action non pas après qu'elle ait été calculé mais directement dans le réseau de neurones. En effet, on peut perturber légèrement les poids du réseau de neurone de l'acteur avant de lui soumettre l'état **state**. L'action résultante serait ainsi bruitée différemment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40388e3d-3cd6-423d-9439-942c1c6ded1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Bruit(Enum):\n",
    "    Gaussien = 1\n",
    "    OU = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430f30d-6010-47ed-8023-f43efd653882",
   "metadata": {},
   "source": [
    "# Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d0c91-e7ae-4c18-9cd6-f67bee10fdd5",
   "metadata": {},
   "source": [
    "Pour implémenter l'algorithme DDPG, il faut utiliser un buffer. Le buffer est un élément important et utile dans l'algorithme DDPG car il permet d'éviter les problèmes de corrélation entre les transitions, améliore la stabilité de l'apprentissage et permet de minimiser les effets de l'instabilité des mises à jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "feb84f6b-ff42-4f7d-8220-dbb0bf9f9d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Return a batch from the buffer\n",
    "    def getBatch(self):\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices], dtype = tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afa990-9bda-4295-b6af-cc1bd0992efe",
   "metadata": {},
   "source": [
    "# Acteur et Critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59827378-3242-47c1-85a6-3be82099e99e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (940569783.py, line 37)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [7]\u001b[0;36m\u001b[0m\n\u001b[0;31m    def policy(self, state):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "class Actor : \n",
    "    def __init__(self, lr, tau = 0.005, bruit = Bruit.Gaussien, std_dev = 0.25):\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.model = self.make_actor()\n",
    "        self.target = self.make_actor()\n",
    "        self.bruit = bruit\n",
    "        self.std_dev = std_dev\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr)\n",
    "        # Making the weights equal initially\n",
    "        self.target.set_weights(self.model.get_weights())\n",
    "        \n",
    "        if bruit == Bruit.OU:\n",
    "            self.noise_generator = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "        elif bruit == Bruit.Gaussien: \n",
    "            self.noise_generator = lambda : np.array([np.random.randn()*self.std_dev])\n",
    "        else: \n",
    "            raise Exception(\"Bruit non attendu\")\n",
    "\n",
    "    def make_actor(self):\n",
    "        # Initialize weights between -3e-3 and 3-e3\n",
    "        last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "        inputs = layers.Input(shape=(num_states,))\n",
    "        out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "        out = layers.Dense(256, activation=\"relu\")(out)\n",
    "        outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "        # Our upper bound is 2.0 for Pendulum.\n",
    "        outputs = outputs * upper_bound\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        return model\n",
    "    \n",
    "    def policy_parameter_space(self, state):\n",
    "        \n",
    "    \n",
    "    def policy(self, state):\n",
    "        tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        sampled_actions = tf.squeeze(self.model(tf_state))\n",
    "        sampled_actions = sampled_actions.numpy() + self.noise_generator()\n",
    "\n",
    "        # We make sure action is within bounds\n",
    "        legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "        return [np.squeeze(legal_action)]\n",
    "    \n",
    "    def update(self, batch, critic):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch = batch        \n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.model(state_batch, training=True)\n",
    "            critic_value = critic.model([state_batch, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "    # This update target parameters slowly\n",
    "    # Based on rate `tau`, which is much less than one.\n",
    "    @tf.function\n",
    "    def update_target(self):\n",
    "        for (a, b) in zip(self.target.variables, self.model.variables):\n",
    "            a.assign(b * self.tau + a * (1 - self.tau))\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, lr, tau = 0.005):\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.model = self.make_critic()\n",
    "        self.target = self.make_critic()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "        # Making the weights equal initially\n",
    "        self.target.set_weights(self.model.get_weights())\n",
    "\n",
    "\n",
    "    def make_critic(self):\n",
    "        # State as input\n",
    "        state_input = layers.Input(shape=(num_states))\n",
    "        state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "        state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "        # Action as input\n",
    "        action_input = layers.Input(shape=(num_actions))\n",
    "        action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "        # Both are passed through seperate layer before concatenating\n",
    "        concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "        out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "        out = layers.Dense(256, activation=\"relu\")(out)\n",
    "        outputs = layers.Dense(1)(out)\n",
    "\n",
    "        # Outputs single value for give state-action\n",
    "        model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def update(self, batch, actor):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch = batch\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = actor.target(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * self.target(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.model.trainable_variables)\n",
    "        )\n",
    "    # This update target parameters slowly\n",
    "    # Based on rate `tau`, which is much less than one.\n",
    "    @tf.function\n",
    "    def update_target(self):\n",
    "        for (a, b) in zip(self.target.variables, self.model.variables):\n",
    "            a.assign(b * self.tau + a * (1 - self.tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6b195-a712-4171-b810-a1b9ead9a356",
   "metadata": {},
   "source": [
    "# Initialisation des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd01335-dd1a-4ba3-9041-05e0829266b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, gamma, actor_lr = 0.001, critic_lr = 0.002, tau = 0.005, total_episodes = 100, \n",
    "                 buffer_capacity = 10000, batch_size = 64, bruit = Bruit.Gaussien, std_dev = None):\n",
    "        self.gamma = gamma\n",
    "        self.actor = Actor(actor_lr, tau, bruit, std_dev)\n",
    "        self.critic = Critic(critic_lr, tau)\n",
    "        self.buffer = ReplayBuffer(buffer_capacity, batch_size)\n",
    "        self.total_episodes = total_episodes\n",
    "        self.episode_rewards = []\n",
    "        self.avg_rewards = []\n",
    "    \n",
    "    def train(self, debug = True):\n",
    "        for ep in range(total_episodes):\n",
    "            prev_state, _ = env.reset()\n",
    "            episodic_reward = 0\n",
    "            step = 0\n",
    "            while True:\n",
    "                action = self.actor.policy(prev_state)\n",
    "\n",
    "                state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "                self.buffer.record((prev_state, action, reward, state))\n",
    "                episodic_reward += reward\n",
    "\n",
    "                batch = self.buffer.getBatch()\n",
    "                self.critic.update(batch, self.actor)\n",
    "                self.actor.update(batch, self.critic)\n",
    "                \n",
    "                self.critic.update_target()\n",
    "                self.actor.update_target()\n",
    "\n",
    "                # End this episode when `done` is True\n",
    "                if done or truncated:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "                step += 1\n",
    "\n",
    "            self.episode_rewards.append(episodic_reward)\n",
    "\n",
    "            # Mean of last 40 episodes\n",
    "            avg_reward = np.mean(self.episode_rewards[-40:])\n",
    "            if debug : \n",
    "                print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "            self.avg_rewards.append(avg_reward)\n",
    "        print(\"Entrainement terminé\")\n",
    "    \n",
    "    \n",
    "    def run(self):\n",
    "        for ep in range(total_episodes):\n",
    "            prev_state, _ = env.reset()\n",
    "            while True:\n",
    "                action = actor.policy(prev_state, ou_noise)\n",
    "                state, _, done, truncated, _ = env.step(action)\n",
    "                if done or truncated:\n",
    "                    break\n",
    "                prev_state = state\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.plot(avg_reward_list)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "        plt.show() \n",
    "        \n",
    "    def save_data(self, filename = None):\n",
    "        import pickle\n",
    "        filename = filename if not None else f\"Pendulum/results/DDPG_bs{self.buffer.batch_size}_alr{self.actor.lr}_clr{self.critic.lr}_br{self.actor.bruit}_dev{self.actor.std_dev}.pickle\"\n",
    "        if self.avg_rewards or self.episode_rewards:\n",
    "            with open(filename, \"wb\") as fichier:\n",
    "                pickle.dump({\n",
    "                    \"parameters\": {\n",
    "                        \"gamma\" : self.gamma,\n",
    "                        \"tau\" : self.actor.tau,\n",
    "                        \"actor_lr\" : self.actor.lr,\n",
    "                        \"critic_lr\" : self.critic.lr,\n",
    "                        \"bruit\" : self.actor.bruit,\n",
    "                        \"std_dev\" : self.actor.std_dev,\n",
    "                        \"batch_size\" : self.buffer.batch_size\n",
    "                    },\n",
    "                    \"data\": {\n",
    "                        \"episode_rewards\" : self.episode_rewards\n",
    "                    }\n",
    "                }, fichier)\n",
    "    def load_data(self, batch_size, actor_lr, critic_lr, bruit, std_dev):\n",
    "        import pickle\n",
    "        with open(f\"Pendulum/results/DDPG_bs{batch_size}_alr{actor_lr}_clr{critic_lr}_br{bruit}_dev{std_dev}.pickle\", \"rb\") as fichier:\n",
    "            data = pickle.load(fichier)\n",
    "            return data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5b977-e2e0-4751-9236-fef1ec3035ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "std_dev = 0.25\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "batch_size = 64\n",
    "\n",
    "#my_ddpg = DDPG(gamma, actor_lr, critic_lr, tau, total_episodes, bruit = Bruit.OU, std_dev = std_dev)\n",
    "#my_ddpg.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a6f16-b977-407f-8baa-c0e34dead65d",
   "metadata": {},
   "source": [
    "Jouons une partie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc4f8d5-6a53-4ca9-9f92-f69a524b3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(filename, data):\n",
    "    import pickle\n",
    "    with open(f\"Pendulum/results/{filename}\", \"wb\") as fichier:\n",
    "        pickle.dump(data, fichier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1abb7a-d3ac-48da-9a67-fd265a0e562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.25\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "gamma = 0.99\n",
    "tau = 0.005\n",
    "batch_size = 64\n",
    "ranges = {\n",
    "    \"gamma\" : np.linspace(0.5, 0.999, 6),\n",
    "    \"critic_lr\" : np.linspace(0.0001, 0.1, 6),\n",
    "    \"actor_lr\" : np.linspace(0.0001, 0.1, 6),\n",
    "    \"tau\" : np.linspace(0.0001, 0.05, 6),\n",
    "    \"batch_size\" : np.arange(16, 128, 16),\n",
    "    \"std_dev\": np.linspace(0.1, 0.5, 6)\n",
    "}\n",
    "total_simulations = 46\n",
    "current_simulation = 1\n",
    "\n",
    "for gamma in ranges[\"gamma\"] : \n",
    "    print(f\"Simulation {current_simulation} / {total_simulations} en cours\")\n",
    "    ddpg = DDPG(gamma, actor_lr, critic_lr, tau, total_episodes, batch_size=batch_size, bruit = Bruit.Gaussien, std_dev = std_dev)\n",
    "    ddpg.train(debug = False)\n",
    "    test = {\n",
    "        \"parameters\": {\n",
    "            \"gamma\" : gamma,\n",
    "            \"tau\" : tau,\n",
    "            \"actor_lr\" : actor_lr,\n",
    "            \"critic_lr\" : critic_lr,\n",
    "            \"bruit\" : Bruit.Gaussien,\n",
    "            \"std_dev\" : std_dev,\n",
    "            \"batch_size\" : batch_size\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"episode_rewards\" : ddpg.episode_rewards\n",
    "        }\n",
    "    }\n",
    "    data_to_save.append(test)\n",
    "    print(f\"Simulation {current_simulation} / {total_simulations} terminée\")\n",
    "    current_simulation += 1\n",
    "save_data(\"DDPG_range_gamma\", data_to_save)\n",
    "data_to_save = []\n",
    "\n",
    "for critic_lr in ranges[\"critic_lr\"] : \n",
    "    print(f\"Simulation {current_simulation} / {total_simulations} en cours\")\n",
    "    ddpg = DDPG(gamma, actor_lr, critic_lr, tau, total_episodes, batch_size=batch_size, bruit = Bruit.Gaussien, std_dev = std_dev)\n",
    "    ddpg.train(debug = False)\n",
    "    test = {\n",
    "        \"parameters\": {\n",
    "            \"gamma\" : gamma,\n",
    "            \"tau\" : tau,\n",
    "            \"actor_lr\" : actor_lr,\n",
    "            \"critic_lr\" : critic_lr,\n",
    "            \"bruit\" : Bruit.Gaussien,\n",
    "            \"std_dev\" : std_dev,\n",
    "            \"batch_size\" : batch_size\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"episode_rewards\" : ddpg.episode_rewards\n",
    "        }\n",
    "    }\n",
    "    data_to_save.append(test)\n",
    "    print(f\"Simulation {current_simulation} / {total_simulations} terminée\")\n",
    "    current_simulation += 1\n",
    "\n",
    "save_data(\"DDPG_range_critic_lr\", data_to_save)\n",
    "data_to_save = []\n",
    "\n",
    "for actor_lr in ranges[\"actor_lr\"] : \n",
    "    print(f\"Simulation {current_simulation} / {total_simulations} en cours\")\n",
    "    ddpg = DDPG(gamma, actor_lr, critic_lr, tau, total_episodes, batch_size=batch_size, bruit = Bruit.Gaussien, std_dev = std_dev)\n",
    "    ddpg.train(debug = False)\n",
    "    test = {\n",
    "        \"parameters\": {\n",
    "            \"gamma\" : gamma,\n",
    "            \"tau\" : tau,\n",
    "            \"actor_lr\" : actor_lr,\n",
    "            \"critic_lr\" : critic_lr,\n",
    "            \"bruit\" : Bruit.Gaussien,\n",
    "            \"std_dev\" : std_dev,\n",
    "            \"batch_size\" : batch_size\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"episode_rewards\" : ddpg.episode_rewards\n",
    "        }\n",
    "    }\n",
    "    data_to_save.append(test)\n",
    "    print(f\"Simulation {current_simulation} / {total_simulations} terminée\")\n",
    "    current_simulation += 1\n",
    "\n",
    "save_data(\"DDPG_range_actor_lr\", data_to_save)\n",
    "data_to_save = []\n",
    "\n",
    "\n",
    "for tau in ranges[\"tau\"] : \n",
    "    print(f\"Simulation {current_simulation} / {total_simulations} en cours\")\n",
    "    ddpg = DDPG(gamma, actor_lr, critic_lr, tau, total_episodes, batch_size=batch_size, bruit = Bruit.Gaussien, std_dev = std_dev)\n",
    "    ddpg.train(debug = False)\n",
    "    test = {\n",
    "        \"parameters\": {\n",
    "            \"gamma\" : gamma,\n",
    "            \"tau\" : tau,\n",
    "            \"actor_lr\" : actor_lr,\n",
    "            \"critic_lr\" : critic_lr,\n",
    "            \"bruit\" : Bruit.Gaussien,\n",
    "            \"std_dev\" : std_dev,\n",
    "            \"batch_size\" : batch_size\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"episode_rewards\" : ddpg.episode_rewards\n",
    "        }\n",
    "    }\n",
    "    data_to_save.append(test)\n",
    "    print(f\"Simulation {current_simulation} / {total_simulations} terminée\")\n",
    "    current_simulation += 1\n",
    "\n",
    "save_data(\"DDPG_range_tau\", data_to_save)\n",
    "data_to_save = []\n",
    "\n",
    "\n",
    "for batch_size in ranges[\"batch_size\"] : \n",
    "    ddpg = DDPG(gamma, actor_lr, critic_lr, tau, total_episodes, batch_size=batch_size, bruit = Bruit.Gaussien, std_dev = std_dev)\n",
    "    ddpg.train(debug = False)\n",
    "    test = {\n",
    "        \"parameters\": {\n",
    "            \"gamma\" : gamma,\n",
    "            \"tau\" : tau,\n",
    "            \"actor_lr\" : actor_lr,\n",
    "            \"critic_lr\" : critic_lr,\n",
    "            \"bruit\" : Bruit.Gaussien,\n",
    "            \"std_dev\" : std_dev,\n",
    "            \"batch_size\" : batch_size\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"episode_rewards\" : ddpg.episode_rewards\n",
    "        }\n",
    "    }\n",
    "    data_to_save.append(test)\n",
    "    print(f\"Simulation {current_simulation} / {total_simulations} terminée\")\n",
    "    current_simulation += 1\n",
    "\n",
    "save_data(\"DDPG_range_batch_size\", data_to_save)\n",
    "data_to_save = []\n",
    "\n",
    "\n",
    "for std_dev in ranges[\"std_dev\"] : \n",
    "    ddpg = DDPG(gamma, actor_lr, critic_lr, tau, total_episodes, batch_size=batch_size, bruit = Bruit.OU, std_dev = std_dev)\n",
    "    ddpg.train(debug = False)\n",
    "    test = {\n",
    "        \"parameters\": {\n",
    "            \"gamma\" : gamma,\n",
    "            \"tau\" : tau,\n",
    "            \"actor_lr\" : actor_lr,\n",
    "            \"critic_lr\" : critic_lr,\n",
    "            \"bruit\" : Bruit.OU,\n",
    "            \"std_dev\" : std_dev,\n",
    "            \"batch_size\" : batch_size\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"episode_rewards\" : ddpg.episode_rewards\n",
    "        }\n",
    "    }\n",
    "    data_to_save.append(test)\n",
    "    print(f\"Simulation {current_simulation} / {total_simulations} terminée\")\n",
    "    current_simulation += 1\n",
    "\n",
    "save_data(\"DDPG_range_std_dev_ou_noise\", data_to_save)\n",
    "data_to_save = []\n",
    "\n",
    "\n",
    "    \n",
    "for std_dev in ranges[\"std_dev\"] : \n",
    "    print(f\"Simulation {current_simulation} / {total_simulations} en cours\")\n",
    "    ddpg = DDPG(gamma, actor_lr, critic_lr, tau, total_episodes, batch_size=batch_size, bruit = Bruit.Gaussien, std_dev = std_dev)\n",
    "    ddpg.train(debug = False)\n",
    "    test = {\n",
    "        \"parameters\": {\n",
    "            \"gamma\" : gamma,\n",
    "            \"tau\" : tau,\n",
    "            \"actor_lr\" : actor_lr,\n",
    "            \"critic_lr\" : critic_lr,\n",
    "            \"bruit\" : Bruit.Gaussien,\n",
    "            \"std_dev\" : std_dev,\n",
    "            \"batch_size\" : batch_size\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"episode_rewards\" : ddpg.episode_rewards\n",
    "        }\n",
    "    }\n",
    "    data_to_save.append(test)\n",
    "    print(f\"Simulation {current_simulation} / {total_simulations} terminée\")\n",
    "    current_simulation += 1\n",
    "\n",
    "save_data(\"DDPG_range_std_dev_gaussian_noise\", data_to_save)\n",
    "data_to_save = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b3f32-1c98-4a99-bfc7-e6860e1e25a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "actor_model.save_weights(\"pendulum_actor.h5\")\n",
    "critic_model.save_weights(\"pendulum_critic.h5\")\n",
    "\n",
    "target_actor.save_weights(\"pendulum_target_actor.h5\")\n",
    "target_critic.save_weights(\"pendulum_target_critic.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
