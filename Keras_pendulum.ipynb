{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43d561c7-f228-47fe-b238-45ed84949b65",
   "metadata": {},
   "source": [
    "# Pendulum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789df808-d345-48c8-8d5a-473a96119b5f",
   "metadata": {},
   "source": [
    "Dans ce notebook, on s'intéressera à la résolution du problème du Pendulum disponible sur la plateforme gymnasium d'OpenAI :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8fedd1-a500-4b16-beb3-d6ec7e8ca5bf",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src = \"https://www.gymlibrary.dev/_images/pendulum.gif\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0038671b-c71f-4983-8bad-b8343dce3e2f",
   "metadata": {},
   "source": [
    "Pour résoudre cet problème, nous allons nous intéresser à l'algorithme de Deep Deterministic Gradient Policy (DDPG)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "247d7a62-e4f7-4dfb-97ef-03c94777445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ab15ca-c8dc-49c1-8e10-d78dd2804d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"Pendulum-v1\"\n",
    "env = gym.make(problem, render_mode = \"human\")\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3279df8f-88f0-4608-9036-a007cd9b58f6",
   "metadata": {},
   "source": [
    "On observe que le problème est décrit à chaque instant par un état contenant 3 variables et une action compris entre [-2, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e56850c-1770-4bd7-9ae2-c5d555e36423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille d'un état : \t 3\n",
      "Taille d'une action : \t 1\n",
      "Bornes de l'action : \t [-2.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Taille d'un état : \\t {num_states}\")\n",
    "print(f\"Taille d'une action : \\t {num_actions}\")\n",
    "print(f\"Bornes de l'action : \\t [{lower_bound}, {upper_bound}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b50646-6f2e-4f11-8b42-f1d621455ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430f30d-6010-47ed-8023-f43efd653882",
   "metadata": {},
   "source": [
    "# Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d0c91-e7ae-4c18-9cd6-f67bee10fdd5",
   "metadata": {},
   "source": [
    "Pour implémenter l'algorithme DDPG, il faut utiliser un buffer. Le buffer est un élément important et utile dans l'algorithme DDPG car il permet d'éviter les problèmes de corrélation entre les transitions, améliore la stabilité de l'apprentissage et permet de minimiser les effets de l'instabilité des mises à jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "feb84f6b-ff42-4f7d-8220-dbb0bf9f9d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Return a batch from the buffer\n",
    "    def getBatch(self):\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "        \n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices], dtype = tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afa990-9bda-4295-b6af-cc1bd0992efe",
   "metadata": {},
   "source": [
    "# Acteur et Critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59827378-3242-47c1-85a6-3be82099e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor : \n",
    "    def __init__(self, lr, tau = 0.005):\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.model = self.make_actor()\n",
    "        self.target = self.make_actor()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr)\n",
    "        # Making the weights equal initially\n",
    "        self.target.set_weights(self.model.get_weights())\n",
    "\n",
    "    def make_actor(self):\n",
    "        # Initialize weights between -3e-3 and 3-e3\n",
    "        last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "        inputs = layers.Input(shape=(num_states,))\n",
    "        out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "        out = layers.Dense(256, activation=\"relu\")(out)\n",
    "        outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "        # Our upper bound is 2.0 for Pendulum.\n",
    "        outputs = outputs * upper_bound\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        return model\n",
    "    \n",
    "    def policy(self, state, noise_object):\n",
    "        tf_state = tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "        sampled_actions = tf.squeeze(self.model(tf_state))\n",
    "        #noise = noise_object()\n",
    "        # Adding noise to action\n",
    "        #sampled_actions = sampled_actions.numpy() + noise\n",
    "        sampled_actions = sampled_actions.numpy() +  np.array([np.random.randn()*0.25])\n",
    "\n",
    "\n",
    "        # We make sure action is within bounds\n",
    "        legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "        return [np.squeeze(legal_action)]\n",
    "    \n",
    "    def update(self, batch, critic):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch = batch        \n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.model(state_batch, training=True)\n",
    "            critic_value = critic.model([state_batch, actions], training=True)\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "    # This update target parameters slowly\n",
    "    # Based on rate `tau`, which is much less than one.\n",
    "    @tf.function\n",
    "    def update_target(self):\n",
    "        for (a, b) in zip(self.target.variables, self.model.variables):\n",
    "            a.assign(b * self.tau + a * (1 - self.tau))\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, lr, tau = 0.005):\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.model = self.make_critic()\n",
    "        self.target = self.make_critic()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "        # Making the weights equal initially\n",
    "        self.target.set_weights(self.model.get_weights())\n",
    "\n",
    "\n",
    "    def make_critic(self):\n",
    "        # State as input\n",
    "        state_input = layers.Input(shape=(num_states))\n",
    "        state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "        state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "        # Action as input\n",
    "        action_input = layers.Input(shape=(num_actions))\n",
    "        action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "        # Both are passed through seperate layer before concatenating\n",
    "        concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "        out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "        out = layers.Dense(256, activation=\"relu\")(out)\n",
    "        outputs = layers.Dense(1)(out)\n",
    "\n",
    "        # Outputs single value for give state-action\n",
    "        model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def update(self, batch, actor):\n",
    "        state_batch, action_batch, reward_batch, next_state_batch = batch\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = actor.target(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * self.target(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.model.trainable_variables)\n",
    "        )\n",
    "    # This update target parameters slowly\n",
    "    # Based on rate `tau`, which is much less than one.\n",
    "    @tf.function\n",
    "    def update_target(self):\n",
    "        for (a, b) in zip(self.target.variables, self.model.variables):\n",
    "            a.assign(b * self.tau + a * (1 - self.tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6b195-a712-4171-b810-a1b9ead9a356",
   "metadata": {},
   "source": [
    "# Initialisation des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcd01335-dd1a-4ba3-9041-05e0829266b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPG:\n",
    "    def __init__(self, gamma, actor_lr = 0.001, critic_lr = 0.002, tau = 0.005, total_episodes = 100, buffer_capacity = 10000, batch_size = 64):\n",
    "        self.gamma = gamma\n",
    "        self.actor = Actor(actor_lr, tau)\n",
    "        self.critic = Critic(critic_lr, tau)\n",
    "        self.buffer = ReplayBuffer(buffer_capacity, batch_size)\n",
    "        self.total_episodes = total_episodes\n",
    "        self.episode_rewards = []\n",
    "        self.avg_rewards = []\n",
    "    \n",
    "    def train(self, debug = True):\n",
    "        for ep in range(total_episodes):\n",
    "            prev_state, _ = env.reset()\n",
    "            episodic_reward = 0\n",
    "            step = 0\n",
    "            while True:\n",
    "                action = self.actor.policy(prev_state, ou_noise)\n",
    "\n",
    "                state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "                self.buffer.record((prev_state, action, reward, state))\n",
    "                episodic_reward += reward\n",
    "\n",
    "                batch = self.buffer.getBatch()\n",
    "                self.critic.update(batch, self.actor)\n",
    "                self.actor.update(batch, self.critic)\n",
    "                \n",
    "                self.critic.update_target()\n",
    "                self.actor.update_target()\n",
    "\n",
    "                # End this episode when `done` is True\n",
    "                if done or truncated:\n",
    "                    break\n",
    "\n",
    "                prev_state = state\n",
    "                step += 1\n",
    "\n",
    "            self.episode_rewards.append(episodic_reward)\n",
    "\n",
    "            # Mean of last 40 episodes\n",
    "            avg_reward = np.mean(self.episode_rewards[-40:])\n",
    "            if debug : \n",
    "                print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "            self.avg_rewards.append(avg_reward)\n",
    "        print(\"Entrainement terminé\")\n",
    "    \n",
    "    \n",
    "    def run(self):\n",
    "        for ep in range(total_episodes):\n",
    "            prev_state, _ = env.reset()\n",
    "            while True:\n",
    "                action = actor.policy(prev_state, ou_noise)\n",
    "                state, _, done, truncated, _ = env.step(action)\n",
    "                if done or truncated:\n",
    "                    break\n",
    "                prev_state = state\n",
    "    \n",
    "    def plot(self):\n",
    "        plt.plot(avg_reward_list)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f5b977-e2e0-4751-9236-fef1ec3035ec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 01:27:23.977345: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-19 01:27:24.120690: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -1350.0918586899695\n",
      "Episode * 1 * Avg Reward is ==> -1553.7338162411984\n",
      "Episode * 2 * Avg Reward is ==> -1500.042454370541\n",
      "Episode * 3 * Avg Reward is ==> -1513.9088498479396\n",
      "Episode * 4 * Avg Reward is ==> -1510.5317278336454\n",
      "Episode * 5 * Avg Reward is ==> -1514.228403098544\n"
     ]
    }
   ],
   "source": [
    "std_dev = 0.25\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "my_ddpg = DDPG(gamma, actor_lr, critic_lr, tau, total_episodes)\n",
    "my_ddpg.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5442e-e46f-4b05-a4ee-9af6ae415190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "\n",
    "# Takes about 4 min to train\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state, _ = env.reset()\n",
    "    episodic_reward = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        # Uncomment this to see the Actor in action\n",
    "        # But not in a python notebook.\n",
    "        # env.render()\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "        action = actor.policy(tf_prev_state, ou_noise)\n",
    "        \n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "        \n",
    "        batch = buffer.getBatch()\n",
    "        critic.update(batch, actor)\n",
    "        actor.update(batch, critic)\n",
    "\n",
    "        \n",
    "        update_target(actor.target.variables, actor.model.variables, tau)\n",
    "        update_target(critic.target.variables, critic.model.variables, tau)\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "        step += 1\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a6f16-b977-407f-8baa-c0e34dead65d",
   "metadata": {},
   "source": [
    "Jouons une partie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef1b94-cf98-4f22-a5af-2c965504db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "state =  tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "while True : \n",
    "    action = policy(state, ou_noise)\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    state =  tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "    if done or truncated : \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b3f32-1c98-4a99-bfc7-e6860e1e25a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "actor_model.save_weights(\"pendulum_actor.h5\")\n",
    "critic_model.save_weights(\"pendulum_critic.h5\")\n",
    "\n",
    "target_actor.save_weights(\"pendulum_target_actor.h5\")\n",
    "target_critic.save_weights(\"pendulum_target_critic.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
