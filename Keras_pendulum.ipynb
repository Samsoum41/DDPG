{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "247d7a62-e4f7-4dfb-97ef-03c94777445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4505284-e572-4dd2-a567-9090caeb8e06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ab15ca-c8dc-49c1-8e10-d78dd2804d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of State Space ->  3\n",
      "Size of Action Space ->  1\n",
      "Max Value of Action ->  2.0\n",
      "Min Value of Action ->  -2.0\n"
     ]
    }
   ],
   "source": [
    "problem = \"Pendulum-v1\"\n",
    "env = gym.make(problem, render_mode = \"human\")\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39b50646-6f2e-4f11-8b42-f1d621455ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430f30d-6010-47ed-8023-f43efd653882",
   "metadata": {},
   "source": [
    "# Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8d0c91-e7ae-4c18-9cd6-f67bee10fdd5",
   "metadata": {},
   "source": [
    "Pour implémenter l'algorithme DDPG, il faut utiliser un buffer. Le buffer est un élément important et utile dans l'algorithme DDPG car il permet d'éviter les problèmes de corrélation entre les transitions, améliore la stabilité de l'apprentissage et permet de minimiser les effets de l'instabilité des mises à jour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "feb84f6b-ff42-4f7d-8220-dbb0bf9f9d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=64):\n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.buffer_counter += 1\n",
    "\n",
    "    # Return a batch from the buffer\n",
    "    def getBatch(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices], dtype = tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46afa990-9bda-4295-b6af-cc1bd0992efe",
   "metadata": {},
   "source": [
    "# Acteur et Critique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59827378-3242-47c1-85a6-3be82099e99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor : \n",
    "    def __init__(self, lr, tau = 0.005):\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.model = self.make_actor()\n",
    "        self.target = self.make_actor()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr)\n",
    "        # Making the weights equal initially\n",
    "        self.target.set_weights(self.model.get_weights())\n",
    "\n",
    "    def make_actor(self):\n",
    "        # Initialize weights between -3e-3 and 3-e3\n",
    "        last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "        inputs = layers.Input(shape=(num_states,))\n",
    "        out = layers.Dense(256, activation=\"relu\")(inputs)\n",
    "        out = layers.Dense(256, activation=\"relu\")(out)\n",
    "        outputs = layers.Dense(1, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "\n",
    "        # Our upper bound is 2.0 for Pendulum.\n",
    "        outputs = outputs * upper_bound\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "        return model\n",
    "    \n",
    "    def policy(self, state, noise_object):\n",
    "        sampled_actions = tf.squeeze(self.model(state))\n",
    "        #noise = noise_object()\n",
    "        # Adding noise to action\n",
    "        #sampled_actions = sampled_actions.numpy() + noise\n",
    "        sampled_actions = sampled_actions.numpy() +  np.array([np.random.randn()*0.25])\n",
    "\n",
    "\n",
    "        # We make sure action is within bounds\n",
    "        legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "        return [np.squeeze(legal_action)]\n",
    "    \n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, critic):\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = self.model(state_batch, training=True)\n",
    "            critic_value = critic.model([state_batch, actions], training=True)\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(actor_grad, self.model.trainable_variables)\n",
    "        )\n",
    "        \n",
    "    # This update target parameters slowly\n",
    "    # Based on rate `tau`, which is much less than one.\n",
    "    @tf.function\n",
    "    def update_target():\n",
    "        for (a, b) in zip(self.target.variables, self.model.variables):\n",
    "            a.assign(b * self.tau + a * (1 - self.tau))\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, lr, tau = 0.005):\n",
    "        self.lr = lr\n",
    "        self.tau = tau\n",
    "        self.model = self.make_critic()\n",
    "        self.target = self.make_critic()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "        # Making the weights equal initially\n",
    "        self.target.set_weights(self.model.get_weights())\n",
    "\n",
    "\n",
    "    def make_critic(self):\n",
    "        # State as input\n",
    "        state_input = layers.Input(shape=(num_states))\n",
    "        state_out = layers.Dense(16, activation=\"relu\")(state_input)\n",
    "        state_out = layers.Dense(32, activation=\"relu\")(state_out)\n",
    "\n",
    "        # Action as input\n",
    "        action_input = layers.Input(shape=(num_actions))\n",
    "        action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "        # Both are passed through seperate layer before concatenating\n",
    "        concat = layers.Concatenate()([state_out, action_out])\n",
    "\n",
    "        out = layers.Dense(256, activation=\"relu\")(concat)\n",
    "        out = layers.Dense(256, activation=\"relu\")(out)\n",
    "        outputs = layers.Dense(1)(out)\n",
    "\n",
    "        # Outputs single value for give state-action\n",
    "        model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def update(self, state_batch, action_batch, reward_batch, next_state_batch, actor):\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = actor.target(next_state_batch, training=True)\n",
    "            y = reward_batch + gamma * self.target(\n",
    "                [next_state_batch, target_actions], training=True\n",
    "            )\n",
    "            critic_value = self.model([state_batch, action_batch], training=True)\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(critic_grad, self.model.trainable_variables)\n",
    "        )\n",
    "    # This update target parameters slowly\n",
    "    # Based on rate `tau`, which is much less than one.\n",
    "    @tf.function\n",
    "    def update_target():\n",
    "        for (a, b) in zip(self.target.variables, self.model.variables):\n",
    "            a.assign(b * self.tau + a * (1 - self.tau))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6b195-a712-4171-b810-a1b9ead9a356",
   "metadata": {},
   "source": [
    "# Initialisation des paramètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f24da99-bbd6-4279-ab3d-645461709746",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.002\n",
    "actor_lr = 0.001\n",
    "\n",
    "total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.005\n",
    "\n",
    "buffer = ReplayBuffer(50000, 64)\n",
    "actor = Actor(actor_lr, tau)\n",
    "critic = Critic(critic_lr, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcd01335-dd1a-4ba3-9041-05e0829266b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 00:11:32.937032: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-03-19 00:11:32.943793: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-03-19 00:11:33.044532: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 0 * Avg Reward is ==> -1274.0621085834405\n",
      "Episode * 1 * Avg Reward is ==> -1352.0426915460303\n",
      "Episode * 2 * Avg Reward is ==> -1476.45601246527\n",
      "Episode * 3 * Avg Reward is ==> -1497.6501037370915\n",
      "Episode * 4 * Avg Reward is ==> -1490.8673294394703\n",
      "Episode * 5 * Avg Reward is ==> -1495.8463528843538\n",
      "Episode * 6 * Avg Reward is ==> -1479.3608997260742\n",
      "Episode * 7 * Avg Reward is ==> -1484.692067012213\n",
      "Episode * 8 * Avg Reward is ==> -1437.816638500086\n",
      "Episode * 9 * Avg Reward is ==> -1373.6004297178654\n",
      "Episode * 10 * Avg Reward is ==> -1320.7125349316145\n",
      "Episode * 11 * Avg Reward is ==> -1290.8863671071265\n",
      "Episode * 12 * Avg Reward is ==> -1243.9592661528375\n",
      "Episode * 13 * Avg Reward is ==> -1174.222391985658\n",
      "Episode * 14 * Avg Reward is ==> -1113.8244672145163\n",
      "Episode * 15 * Avg Reward is ==> -1052.7794586045886\n",
      "Episode * 16 * Avg Reward is ==> -1012.9500965090944\n",
      "Episode * 17 * Avg Reward is ==> -963.8590143787509\n",
      "Episode * 18 * Avg Reward is ==> -919.8411018540697\n",
      "Episode * 19 * Avg Reward is ==> -873.9340993751808\n",
      "Episode * 20 * Avg Reward is ==> -848.5806439694253\n",
      "Episode * 21 * Avg Reward is ==> -821.0671498932984\n",
      "Episode * 22 * Avg Reward is ==> -790.8180886966619\n",
      "Episode * 23 * Avg Reward is ==> -762.6766596982817\n",
      "Episode * 24 * Avg Reward is ==> -741.8934341570377\n",
      "Episode * 25 * Avg Reward is ==> -725.8685137107633\n",
      "Episode * 26 * Avg Reward is ==> -703.379613903707\n",
      "Episode * 27 * Avg Reward is ==> -682.5546136317352\n",
      "Episode * 28 * Avg Reward is ==> -663.4399568805238\n",
      "Episode * 29 * Avg Reward is ==> -649.4335772332\n",
      "Episode * 30 * Avg Reward is ==> -628.5258514658917\n",
      "Episode * 31 * Avg Reward is ==> -612.5489324665266\n",
      "Episode * 32 * Avg Reward is ==> -597.807816752083\n",
      "Episode * 33 * Avg Reward is ==> -583.7178365210457\n",
      "Episode * 34 * Avg Reward is ==> -570.6636889142875\n",
      "Episode * 35 * Avg Reward is ==> -558.2964368565121\n",
      "Episode * 36 * Avg Reward is ==> -546.6740240145103\n",
      "Episode * 37 * Avg Reward is ==> -535.6957646279924\n",
      "Episode * 38 * Avg Reward is ==> -524.9456923107238\n",
      "Episode * 39 * Avg Reward is ==> -514.9106511094325\n",
      "Episode * 40 * Avg Reward is ==> -489.0739120760969\n",
      "Episode * 41 * Avg Reward is ==> -456.31846981266443\n",
      "Episode * 42 * Avg Reward is ==> -419.227680638012\n",
      "Episode * 43 * Avg Reward is ==> -383.1157964294007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To store reward history of each episode\n",
    "ep_reward_list = []\n",
    "# To store average reward history of last few episodes\n",
    "avg_reward_list = []\n",
    "\n",
    "std_dev = 0.5\n",
    "start_steps = 300\n",
    "max_steps = 10000\n",
    "# Takes about 4 min to train\n",
    "for ep in range(total_episodes):\n",
    "\n",
    "    prev_state, _ = env.reset()\n",
    "    episodic_reward = 0\n",
    "    step = 0\n",
    "    while True:\n",
    "        # Uncomment this to see the Actor in action\n",
    "        # But not in a python notebook.\n",
    "        # env.render()\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "        action = actor.policy(tf_prev_state, ou_noise)\n",
    "        \n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        buffer.record((prev_state, action, reward, state))\n",
    "        episodic_reward += reward\n",
    "        \n",
    "        state_batch, action_batch, reward_batch, next_state_batch = buffer.getBatch()\n",
    "        critic.update(state_batch, action_batch, reward_batch, next_state_batch, actor)\n",
    "        actor.update(state_batch, action_batch, reward_batch, next_state_batch, critic)\n",
    "\n",
    "        \n",
    "        update_target(actor.target.variables, actor.model.variables, tau)\n",
    "        update_target(critic.target.variables, critic.model.variables, tau)\n",
    "\n",
    "        # End this episode when `done` is True\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "        prev_state = state\n",
    "        step += 1\n",
    "\n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    print(\"Episode * {} * Avg Reward is ==> {}\".format(ep, avg_reward))\n",
    "    avg_reward_list.append(avg_reward)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(avg_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Epsiodic Reward\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869a6f16-b977-407f-8baa-c0e34dead65d",
   "metadata": {},
   "source": [
    "Jouons une partie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef1b94-cf98-4f22-a5af-2c965504db61",
   "metadata": {},
   "outputs": [],
   "source": [
    "state, _ = env.reset()\n",
    "state =  tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "while True : \n",
    "    action = policy(state, ou_noise)\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    state =  tf.expand_dims(tf.convert_to_tensor(state), 0)\n",
    "    if done or truncated : \n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b3f32-1c98-4a99-bfc7-e6860e1e25a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "actor_model.save_weights(\"pendulum_actor.h5\")\n",
    "critic_model.save_weights(\"pendulum_critic.h5\")\n",
    "\n",
    "target_actor.save_weights(\"pendulum_target_actor.h5\")\n",
    "target_critic.save_weights(\"pendulum_target_critic.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
